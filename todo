======================= DONE ==============================

*. different shape of f16 mma kernel : done
*. tf32 kernel pass : lhs ldmatrix passed, mma passed, rhs ldmatrix still has bug
*. repack into Matrix : dist_to_fragments, and can be wrapped within kernel, make kernel interface as Matrix (c with float*, will be opt'd latter)
*. refactor API
*. add C-CACHE, set it to choreo
*. matmul_tensor_cores.h, line 69, fix bug, cause: non-normalised loop and symbolic index calculation
*. matmul_tensor_cores_tuned.h, line 151, bug, still slower with <=: rationale, we need compile-time value for shape and index calculation, with constexpr maybe
*. legacy tests for tile
*. test static tile in catch2, as __device__ func
*. config CATCH2 and add test for constexpr design
*. make all correct first, and impose user's behaviour is allowed. no need to force unified helpers

======================= TODO ==============================

*. refactor all cases before porting
*. integrate to choreo

*. use compile-time value for Matrix design, like nalgebra | if problem comes from threadIdx.y these runtime value, try to add const value into API for (loop-extents)
*. pack wmma into Matrix design
*. make one faster kernel, not limit to. (need to pack fragment loading in Matrix design)
*. integrate to choreo // next week
*. add cp.async
*. add f4copy to <=
*. add border check to <=, allow more, allow less, allow tail, use ldmatrix-like design

*. add TODO to baseline accrucy : pending, leave to latter dev
*. handle C reuse issue : we fine, we can work now, and not become bottleneck yet.



================ NEW TODOS =====================
// TODO:
1. fix all other kernels: 1-5 all fine; 
